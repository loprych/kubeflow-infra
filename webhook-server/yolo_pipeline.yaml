# PIPELINE DEFINITION
# Name: yolov8-pipeline
# Description: YOLOv8 pipeline
components:
  comp-compare-models:
    executorLabel: exec-compare-models
    inputDefinitions:
      artifacts:
        trained_model_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        accuracy_threshold:
          defaultValue: 0.03
          isOptional: true
          parameterType: NUMBER_DOUBLE
        base_model_path:
          defaultValue: model/yolov8n.pt
          isOptional: true
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        previous_trained_model_path:
          defaultValue: model/trained_yolo_model.pt
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model_comparison_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        trained_model_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        evaluation_logs_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-inference-model:
    executorLabel: exec-inference-model
    inputDefinitions:
      artifacts:
        trained_model_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        inference_logs_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-package-to-mar:
    executorLabel: exec-package-to-mar
    inputDefinitions:
      artifacts:
        trained_model_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        base_model_path:
          defaultValue: model/yolov8n.pt
          isOptional: true
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        base_mar_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        trained_mar_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-serve:
    executorLabel: exec-serve
    inputDefinitions:
      artifacts:
        base_mar_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        model_comparison_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        trained_mar_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        service_namespace:
          defaultValue: kubeflow-user-example-com
          isOptional: true
          parameterType: STRING
  comp-train:
    executorLabel: exec-train
    inputDefinitions:
      parameters:
        base_model:
          defaultValue: yolov8n.pt
          isOptional: true
          parameterType: STRING
        dataset_path:
          defaultValue: dataset
          isOptional: true
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        trained_model_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        training_logs_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-compare-models:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compare_models
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compare_models(\n    trained_model_input: Input[Artifact],\n\
          \    model_comparison_output: Output[Artifact],\n    minio_endpoint: str,\n\
          \    minio_access_key: str,\n    minio_secret_key: str,\n    minio_bucket:\
          \ str,\n    base_model_path: str = \"model/yolov8n.pt\",\n    previous_trained_model_path:\
          \ str = \"model/trained_yolo_model.pt\",\n    accuracy_threshold: float\
          \ = 0.03  # 3% improvement required\n):\n\n    import os\n    import tempfile\n\
          \    import json\n    from pathlib import Path\n    from ultralytics import\
          \ YOLO\n    from minio import Minio\n    import time\n\n    client = Minio(\n\
          \        minio_endpoint,\n        access_key=minio_access_key,\n       \
          \ secret_key=minio_secret_key,\n        secure=False\n    )\n\n    with\
          \ tempfile.TemporaryDirectory() as temp_dir:\n        temp_dir = Path(temp_dir)\n\
          \        new_model_path = temp_dir / \"new_model.pt\"\n        prev_model_path\
          \ = temp_dir / \"previous_model.pt\"\n        base_model_local_path = temp_dir\
          \ / \"base_model.pt\"\n        validation_images_dir = temp_dir / \"validation_images\"\
          \n        validation_images_dir.mkdir(exist_ok=True)\n        comparison_results_path\
          \ = temp_dir / \"comparison_results.json\"\n\n        # Copy the new model\
          \ from input\n        with open(trained_model_input.path, 'rb') as in_f:\n\
          \            with open(new_model_path, 'wb') as out_f:\n               \
          \ out_f.write(in_f.read())\n\n        # Check if this is the first training\
          \ run or if previous trained model exists\n        previous_model_exists\
          \ = True\n        try:\n            # Use timestamp to check if the previous\
          \ trained model is not the same as current one\n            prev_stats =\
          \ client.stat_object(minio_bucket, previous_trained_model_path)\n      \
          \      curr_time = time.time()\n            # If the model was created in\
          \ the last hour, it's likely from the current run\n            # In that\
          \ case, compare against the base model\n            if (curr_time - prev_stats.last_modified.timestamp())\
          \ < 3600:\n                print(\"Previous trained model is likely from\
          \ current run, comparing against base model\")\n                client.fget_object(\n\
          \                    minio_bucket,\n                    base_model_path,\n\
          \                    str(prev_model_path)\n                )\n         \
          \   else:\n                client.fget_object(\n                    minio_bucket,\n\
          \                    previous_trained_model_path,\n                    str(prev_model_path)\n\
          \                )\n        except Exception as e:\n            print(f\"\
          Previous trained model not found: {e}\")\n            print(\"Will compare\
          \ against base model instead\")\n            try:\n                client.fget_object(\n\
          \                    minio_bucket,\n                    base_model_path,\n\
          \                    str(prev_model_path)\n                )\n         \
          \   except Exception as e2:\n                print(f\"Base model not found\
          \ either: {e2}\")\n                previous_model_exists = False\n\n   \
          \     # Get validation images\n        val_images = [\"zidane.jpg\", \"\
          bus.jpg\"]\n        for img in val_images:\n            local_img = validation_images_dir\
          \ / img\n            try:\n                client.fget_object(\n       \
          \             minio_bucket,\n                    f\"images/{img}\",\n  \
          \                  str(local_img)\n                )\n            except\
          \ Exception as e:\n                print(f\"Warning: Could not fetch validation\
          \ image {img}: {e}\")\n                # Try to use demo images from ultralytics\n\
          \                import shutil\n                from ultralytics.utils.downloads\
          \ import download\n                download(f\"https://ultralytics.com/images/{img}\"\
          , str(local_img))\n\n        # Load new model\n        new_model = YOLO(str(new_model_path))\n\
          \n        # Evaluate new model\n        new_model_metrics = {}\n       \
          \ for img_name in val_images:\n            img_path = validation_images_dir\
          \ / img_name\n            if not img_path.exists():\n                print(f\"\
          Skipping missing image: {img_name}\")\n                continue\n\n    \
          \        results = new_model(str(img_path))\n            # Extract confidence\
          \ scores and metrics\n            boxes = results[0].boxes\n           \
          \ new_model_metrics[img_name] = {\n                'num_detections': len(boxes),\n\
          \                'avg_confidence': float(boxes.conf.mean()) if len(boxes)\
          \ > 0 else 0,\n                'max_confidence': float(boxes.conf.max())\
          \ if len(boxes) > 0 else 0\n            }\n\n        comparison_result =\
          \ {\n            'new_model_metrics': new_model_metrics,\n            'previous_model_metrics':\
          \ {},\n            'is_better': True,  # Default to True if no previous\
          \ model exists\n            'improvement': 0,\n            'message': 'First\
          \ model, no comparison available'\n        }\n\n        if previous_model_exists:\n\
          \            # Load previous model\n            prev_model = YOLO(str(prev_model_path))\n\
          \n            # Evaluate previous model\n            prev_model_metrics\
          \ = {}\n            for img_name in val_images:\n                img_path\
          \ = validation_images_dir / img_name\n                if not img_path.exists():\n\
          \                    continue\n\n                results = prev_model(str(img_path))\n\
          \                boxes = results[0].boxes\n                prev_model_metrics[img_name]\
          \ = {\n                    'num_detections': len(boxes),\n             \
          \       'avg_confidence': float(boxes.conf.mean()) if len(boxes) > 0 else\
          \ 0,\n                    'max_confidence': float(boxes.conf.max()) if len(boxes)\
          \ > 0 else 0\n                }\n\n            comparison_result['previous_model_metrics']\
          \ = prev_model_metrics\n\n            # Calculate metrics:\n           \
          \ # 1. Average confidence across all images\n            # 2. Total detections\
          \ (weighted by confidence)\n\n            # Get valid images that both models\
          \ processed\n            valid_images = set(new_model_metrics.keys()) &\
          \ set(prev_model_metrics.keys())\n            if not valid_images:\n   \
          \             comparison_result['message'] = \"No common images to compare\
          \ models\"\n                comparison_result['is_better'] = True\n    \
          \        else:\n                # Calculate weighted detection score (num_detections\
          \ * avg_confidence)\n                new_score = sum(\n                \
          \    new_model_metrics[img]['num_detections'] * new_model_metrics[img]['avg_confidence']\n\
          \                    for img in valid_images\n                ) / len(valid_images)\n\
          \n                prev_score = sum(\n                    prev_model_metrics[img]['num_detections']\
          \ * prev_model_metrics[img]['avg_confidence']\n                    for img\
          \ in valid_images\n                ) / len(valid_images)\n\n           \
          \     improvement = (new_score - prev_score) / max(prev_score, 0.001)\n\n\
          \                comparison_result['improvement'] = float(improvement)\n\
          \                comparison_result['is_better'] = improvement >= accuracy_threshold\n\
          \n                if comparison_result['is_better']:\n                 \
          \   comparison_result['message'] = f\"New model is better by {improvement:.2%}\"\
          \n                else:\n                    comparison_result['message']\
          \ = (\n                        f\"New model does not meet improvement threshold.\
          \ \"\n                        f\"Improvement: {improvement:.2%}, Required:\
          \ {accuracy_threshold:.2%}\"\n                    )\n\n        # Save comparison\
          \ results\n        with open(comparison_results_path, 'w') as f:\n     \
          \       json.dump(comparison_result, f, indent=2)\n\n        # Save to MinIO\
          \ as well\n        client.put_object(\n            minio_bucket,\n     \
          \       \"comparison/model_comparison.json\",\n            open(comparison_results_path,\
          \ 'rb'),\n            length=os.path.getsize(str(comparison_results_path))\n\
          \        )\n\n        with open(model_comparison_output.path, 'w') as out_f:\n\
          \            out_f.write(open(comparison_results_path, 'r').read())\n\n\
          \        print(f\"Comparison complete: {comparison_result['message']}\"\
          )\n        print(f\"Model is better: {comparison_result['is_better']}\"\
          )\n\n"
        image: lukoprych/yolov8-pipeline-base:latest
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    trained_model_input: Input[Artifact],\n\
          \    evaluation_logs_output: Output[Artifact],\n    minio_endpoint: str,\n\
          \    minio_access_key: str,\n    minio_secret_key: str,\n    minio_bucket:\
          \ str\n):\n    import tempfile\n    from pathlib import Path\n    from ultralytics\
          \ import YOLO\n    from minio import Minio\n    from datetime import datetime\n\
          \    import os\n\n    client = Minio(\n        minio_endpoint,\n       \
          \ access_key=minio_access_key,\n        secret_key=minio_secret_key,\n \
          \       secure=False\n    )\n\n    with tempfile.TemporaryDirectory() as\
          \ temp_dir:\n        temp_dir = Path(temp_dir)\n        model_local_path\
          \ = temp_dir / \"trained_model.pt\"\n        eval_log_path = temp_dir /\
          \ \"eval_logs.txt\"\n\n        with open(trained_model_input.path, 'rb')\
          \ as in_f:\n            with open(model_local_path, 'wb') as out_f:\n  \
          \              out_f.write(in_f.read())\n\n        image_paths = []\n  \
          \      for img_name in [\"zidane.jpg\", \"bus.jpg\"]:\n            image_local\
          \ = temp_dir / img_name\n            try:\n                client.fget_object(\n\
          \                    minio_bucket,\n                    f\"images/{img_name}\"\
          ,\n                    str(image_local)\n                )\n           \
          \     image_paths.append(image_local)\n            except Exception as e:\n\
          \                print(f\"Warning: Could not download {img_name}: {e}\"\
          )\n                # Try to download from ultralytics\n                try:\n\
          \                    from ultralytics.utils.downloads import download\n\
          \                    download(f\"https://ultralytics.com/images/{img_name}\"\
          , str(image_local))\n                    image_paths.append(image_local)\n\
          \                except Exception as e2:\n                    print(f\"\
          Could not download image from backup source: {e2}\")\n\n        model =\
          \ YOLO(str(model_local_path))\n\n        with open(eval_log_path, 'w') as\
          \ f:\n            f.write(\"=== EVALUATION RESULTS ===\\n\")\n         \
          \   for img_path in image_paths:\n                f.write(f\"\\nEvaluating\
          \ image: {img_path.name}\\n\")\n                results = model(str(img_path))\n\
          \                f.write(f\"Detection results: {results[0].boxes.shape[0]}\
          \ objects found\\n\")\n\n                # Detailed metrics\n          \
          \      boxes = results[0].boxes\n                if len(boxes) > 0:\n  \
          \                  f.write(f\"Average confidence: {boxes.conf.mean().item():.4f}\\\
          n\")\n                    f.write(f\"Maximum confidence: {boxes.conf.max().item():.4f}\\\
          n\")\n\n                    # Class distribution\n                    classes\
          \ = boxes.cls.tolist()\n                    names = results[0].names\n \
          \                   class_counts = {}\n                    for cls in classes:\n\
          \                        cls_name = names[int(cls)]\n                  \
          \      class_counts[cls_name] = class_counts.get(cls_name, 0) + 1\n\n  \
          \                  f.write(\"Class distribution:\\n\")\n               \
          \     for cls_name, count in class_counts.items():\n                   \
          \     f.write(f\"  - {cls_name}: {count}\\n\")\n\n                f.write(f\"\
          Results: {results}\\n\")\n                f.write(\"-\" * 40 + \"\\n\")\n\
          \n        # Save logs to output artifact\n        with open(evaluation_logs_output.path,\
          \ 'w') as log_art:\n            log_art.write(open(eval_log_path, 'r').read())\n\
          \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n     \
          \   log_filename = f\"logs/eval_logs_{timestamp}.txt\"\n\n        # Also\
          \ save to MinIO with specific name\n        client.put_object(\n       \
          \     minio_bucket,\n            log_filename,\n            open(eval_log_path,\
          \ 'rb'),\n            length=os.path.getsize(str(eval_log_path))\n     \
          \   )\n\n"
        image: lukoprych/yolov8-pipeline-base:latest
    exec-inference-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - inference_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef inference_model(\n    trained_model_input: Input[Artifact],\n\
          \    inference_logs_output: Output[Artifact],\n    minio_endpoint: str,\n\
          \    minio_access_key: str,\n    minio_secret_key: str,\n    minio_bucket:\
          \ str\n):\n    import os\n    import tempfile\n    import time\n    from\
          \ pathlib import Path\n    from ultralytics import YOLO\n    from minio\
          \ import Minio\n    from datetime import datetime\n\n    client = Minio(\n\
          \        minio_endpoint,\n        access_key=minio_access_key,\n       \
          \ secret_key=minio_secret_key,\n        secure=False\n    )\n\n    with\
          \ tempfile.TemporaryDirectory() as temp_dir:\n        temp_dir = Path(temp_dir)\n\
          \        model_local_path = temp_dir / \"trained_model.pt\"\n        inference_log_path\
          \ = temp_dir / \"inference_logs.txt\"\n        result_images_dir = temp_dir\
          \ / \"results\"\n        result_images_dir.mkdir(exist_ok=True)\n\n    \
          \    with open(trained_model_input.path, 'rb') as in_f:\n            with\
          \ open(model_local_path, 'wb') as out_f:\n                out_f.write(in_f.read())\n\
          \n        # Get test images\n        images = [\"zidane.jpg\", \"bus.jpg\"\
          ]\n        local_img_paths = []\n\n        for img in images:\n        \
          \    local_img = temp_dir / img\n            try:\n                client.fget_object(\n\
          \                    minio_bucket,\n                    f\"images/{img}\"\
          ,\n                    str(local_img)\n                )\n             \
          \   local_img_paths.append(local_img)\n            except Exception as e:\n\
          \                print(f\"Warning: Could not download {img}: {e}\")\n  \
          \              # Try to download from ultralytics\n                try:\n\
          \                    from ultralytics.utils.downloads import download\n\
          \                    download(f\"https://ultralytics.com/images/{img}\"\
          , str(local_img))\n                    local_img_paths.append(local_img)\n\
          \                except Exception as e2:\n                    print(f\"\
          Could not download image from backup source: {e2}\")\n\n        model =\
          \ YOLO(str(model_local_path))\n        timestamp = int(time.time())\n\n\
          \        with open(inference_log_path, 'w') as f:\n            f.write(\"\
          === INFERENCE RESULTS ===\\n\")\n            f.write(f\"Timestamp: {timestamp}\\\
          n\")\n            f.write(f\"Model: {model_local_path}\\n\\n\")\n\n    \
          \        for img_path in local_img_paths:\n                f.write(f\"Image:\
          \ {img_path.name}\\n\")\n                # Run inference with visualization\n\
          \                results = model(str(img_path), save=True, save_dir=str(result_images_dir))\n\
          \n                # Save detailed results\n                boxes = results[0].boxes\n\
          \                f.write(f\"Detection count: {len(boxes)}\\n\")\n\n    \
          \            if len(boxes) > 0:\n                    f.write(f\"Average\
          \ confidence: {boxes.conf.mean().item():.4f}\\n\")\n                   \
          \ f.write(f\"Classes detected: {len(set(boxes.cls.tolist()))}\\n\")\n\n\
          \                    # Class distribution with confidence\n            \
          \        classes = boxes.cls.tolist()\n                    confs = boxes.conf.tolist()\n\
          \                    names = results[0].names\n\n                    f.write(\"\
          Detections:\\n\")\n                    for i, (cls, conf) in enumerate(zip(classes,\
          \ confs)):\n                        cls_name = names[int(cls)]\n       \
          \                 f.write(f\"  {i+1}. {cls_name} (confidence: {conf:.4f})\\\
          n\")\n\n                f.write(f\"Raw results: {results}\\n\")\n      \
          \          f.write(\"-\" * 60 + \"\\n\\n\")\n\n                # Upload\
          \ result images to MinIO if they exist\n                result_img = result_images_dir\
          \ / img_path.name\n                if result_img.exists():\n           \
          \         try:\n                        client.fput_object(\n          \
          \                  minio_bucket,\n                            f\"inference/results/{timestamp}_{img_path.name}\"\
          ,\n                            str(result_img)\n                       \
          \ )\n                    except Exception as e:\n                      \
          \  f.write(f\"Failed to upload result image: {e}\\n\")\n\n        # Save\
          \ to artifact output\n        with open(inference_logs_output.path, 'w')\
          \ as log_art:\n            log_art.write(open(inference_log_path, 'r').read())\n\
          \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n     \
          \   log_filename = f\"logs/inference_logs_{timestamp}.txt\"\n\n        #\
          \ Also save to MinIO with specific name\n        client.put_object(\n  \
          \          minio_bucket,\n            log_filename,\n            open(inference_log_path,\
          \ 'rb'),\n            length=os.path.getsize(str(inference_log_path))\n\
          \        )\n\n"
        image: lukoprych/yolov8-pipeline-base:latest
    exec-package-to-mar:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - package_to_mar
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'torch-model-archiver'\
          \ 'torchserve' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef package_to_mar(\n    trained_model_input: Input[Artifact],\n\
          \    trained_mar_output: Output[Artifact],\n    base_mar_output: Output[Artifact],\n\
          \    minio_endpoint: str,\n    minio_access_key: str,\n    minio_secret_key:\
          \ str,\n    minio_bucket: str,\n    base_model_path: str = \"model/yolov8n.pt\"\
          \n):\n    import tempfile\n    from pathlib import Path\n    from minio\
          \ import Minio\n    import subprocess\n    import os\n\n    client = Minio(\n\
          \        minio_endpoint,\n        access_key=minio_access_key,\n       \
          \ secret_key=minio_secret_key,\n        secure=False\n    )\n\n    with\
          \ tempfile.TemporaryDirectory() as temp_dir:\n        temp_dir = Path(temp_dir)\n\
          \        trained_model_path = temp_dir / \"trained_yolo_model.pt\"\n   \
          \     base_model_path_local = temp_dir / \"yolov8n.pt\"\n        model_store\
          \ = temp_dir / \"model-store\"\n        model_store.mkdir(exist_ok=True)\n\
          \        handler_path = temp_dir / \"yolo_handler.py\"\n\n        # Write\
          \ handler\n        with open(handler_path, \"w\") as f:\n            f.write(\"\
          \"\"import logging\nimport os\nfrom collections import Counter\nimport torch\n\
          from torchvision import transforms\nfrom ultralytics import YOLO\nfrom ts.torch_handler.object_detector\
          \ import ObjectDetector\nimport io\nimport base64\nfrom PIL import Image\n\
          \nlogger = logging.getLogger(__name__)\n\ntry:\n    import torch_xla.core.xla_model\
          \ as xm\n    XLA_AVAILABLE = True\nexcept ImportError as error:\n    XLA_AVAILABLE\
          \ = False\n\nclass Yolov8Handler(ObjectDetector):\n    image_processing\
          \ = transforms.Compose([\n        transforms.Resize(640),\n        transforms.CenterCrop(640),\n\
          \        transforms.ToTensor()\n    ])\n\n    def __init__(self):\n    \
          \    super(Yolov8Handler, self).__init__()\n\n    def initialize(self, context):\n\
          \        if torch.cuda.is_available():\n            self.device = torch.device(\"\
          cuda\")\n        elif XLA_AVAILABLE:\n            self.device = xm.xla_device()\n\
          \        else:\n            self.device = torch.device(\"cpu\")\n\n    \
          \    properties = context.system_properties\n        self.manifest = context.manifest\n\
          \        model_dir = properties.get(\"model_dir\")\n        self.model_pt_path\
          \ = None\n        if \"serializedFile\" in self.manifest[\"model\"]:\n \
          \           serialized_file = self.manifest[\"model\"][\"serializedFile\"\
          ]\n            self.model_pt_path = os.path.join(model_dir, serialized_file)\n\
          \        self.model = self._load_torchscript_model(self.model_pt_path)\n\
          \        logger.debug(\"Model file %s loaded successfully\", self.model_pt_path)\n\
          \        self.initialized = True\n\n    def _load_torchscript_model(self,\
          \ model_pt_path):\n        model = YOLO(model_pt_path)\n        model.to(self.device)\n\
          \        return model\n\n    def preprocess(self, requests):\n        images\
          \ = []\n        for data in requests:\n            image = data.get(\"data\"\
          ) or data.get(\"body\")\n            if isinstance(image, str):\n      \
          \          image = base64.b64decode(image)\n            image = Image.open(io.BytesIO(image)).convert('RGB')\n\
          \            image = self.image_processing(image)\n            images.append(image)\n\
          \        return torch.stack(images).to(self.device)\n\n    def inference(self,\
          \ data):\n        results = self.model(data)\n        return results\n\n\
          \    def postprocess(self, res):\n        output = []\n        for data\
          \ in res:\n            classes = data.boxes.cls.tolist()\n            names\
          \ = data.names\n            classes = map(lambda cls: names[int(cls)], classes)\n\
          \            result = Counter(classes)\n            output.append(dict(result))\n\
          \        return output\"\"\")\n\n        # Create requirements file\n  \
          \      requirements_path = temp_dir / \"requirements.txt\"\n        with\
          \ open(requirements_path, \"w\") as f:\n            f.write(\"torch\\nPillow\\\
          nultralytics\\ntorchvision\\n\")\n\n        # Copy trained model\n     \
          \   print(f\"Copying trained model to {trained_model_path}\")\n        with\
          \ open(trained_model_input.path, 'rb') as in_f:\n            with open(trained_model_path,\
          \ 'wb') as out_f:\n                out_f.write(in_f.read())\n\n        if\
          \ not trained_model_path.exists():\n            raise FileNotFoundError(f\"\
          Trained model was not copied to {trained_model_path}\")\n\n        # Get\
          \ base model\n        print(f\"Getting base model from MinIO\")\n      \
          \  client.fget_object(\n            minio_bucket,\n            base_model_path,\n\
          \            str(base_model_path_local)\n        )\n\n        if not base_model_path_local.exists():\n\
          \            raise FileNotFoundError(f\"Base model was not downloaded to\
          \ {base_model_path_local}\")\n\n        # Package trained model\n      \
          \  print(\"Creating trained model MAR file...\")\n        result = subprocess.run([\n\
          \            'torch-model-archiver',\n            '--model-name', 'trained_yolo_model',\
          \ \n            '--version', '1.0',\n            '--serialized-file', str(trained_model_path),\n\
          \            '--handler', str(handler_path),\n            '--requirements-file',\
          \ str(requirements_path),\n            '--export-path', str(model_store),\n\
          \            '--force'\n        ], capture_output=True, text=True)\n\n \
          \       if result.returncode != 0:\n            raise Exception(f\"Error\
          \ creating trained MAR file: {result.stderr}\")\n\n        # Package base\
          \ model\n        print(\"Creating base model MAR file...\")\n        result\
          \ = subprocess.run([\n            'torch-model-archiver',\n            '--model-name',\
          \ 'base_yolo_model', \n            '--version', '1.0',\n            '--serialized-file',\
          \ str(base_model_path_local),\n            '--handler', str(handler_path),\n\
          \            '--requirements-file', str(requirements_path),\n          \
          \  '--export-path', str(model_store),\n            '--force'\n        ],\
          \ capture_output=True, text=True)\n\n        if result.returncode != 0:\n\
          \            raise Exception(f\"Error creating base MAR file: {result.stderr}\"\
          )\n\n        trained_mar_path = model_store / \"trained_yolo_model.mar\"\
          \n        base_mar_path = model_store / \"base_yolo_model.mar\"\n\n    \
          \    if not trained_mar_path.exists() or not base_mar_path.exists():\n \
          \           raise FileNotFoundError(f\"MAR files were not created: {trained_mar_path},\
          \ {base_mar_path}\")\n\n        print(f\"MAR files created at: {trained_mar_path}\
          \ and {base_mar_path}\")\n\n        # Save to MinIO\n        print(\"Saving\
          \ to MinIO...\")\n        client.fput_object(\n            minio_bucket,\n\
          \            \"kserve/model-store/trained_yolo_model.mar\",\n          \
          \  str(trained_mar_path)\n        )\n        client.fput_object(\n     \
          \       minio_bucket,\n            \"kserve/model-store/base_yolo_model.mar\"\
          ,\n            str(base_mar_path)\n        )\n\n        # Save outputs\n\
          \        print(\"Saving outputs...\")\n        with open(trained_mar_output.path,\
          \ 'wb') as out_f:\n            out_f.write(open(trained_mar_path, 'rb').read())\n\
          \        with open(base_mar_output.path, 'wb') as out_f:\n            out_f.write(open(base_mar_path,\
          \ 'rb').read())\n\n"
        image: lukoprych/yolov8-pipeline-base:latest
    exec-serve:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - serve
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kubernetes'\
          \ 'PyYAML' 'minio' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef serve(\n    trained_mar_input: Input[Artifact],\n    base_mar_input:\
          \ Input[Artifact],\n    model_comparison_input: Input[Artifact],\n    minio_endpoint:\
          \ str,\n    minio_access_key: str,\n    minio_secret_key: str,\n    minio_bucket:\
          \ str,\n    service_namespace: str = \"kubeflow-user-example-com\"\n):\n\
          \    import kubernetes\n    import yaml\n    import os\n    import json\n\
          \    from minio import Minio\n    import tempfile\n    from pathlib import\
          \ Path\n    import io\n\n    # Connect to MinIO\n    client = Minio(\n \
          \       minio_endpoint,\n        access_key=minio_access_key,\n        secret_key=minio_secret_key,\n\
          \        secure=False\n    )\n\n    # Read comparison results to determine\
          \ which model to serve\n    with tempfile.TemporaryDirectory() as temp_dir:\n\
          \        temp_dir = Path(temp_dir)\n        comparison_path = temp_dir /\
          \ \"comparison_results.json\"\n        deploy_log_path = temp_dir / \"deployment_logs.txt\"\
          \n        model_path = temp_dir / \"model.mar\"\n\n        with open(model_comparison_input.path,\
          \ 'r') as in_f:\n            with open(comparison_path, 'w') as out_f:\n\
          \                out_f.write(in_f.read())\n\n        with open(comparison_path,\
          \ 'r') as f:\n            comparison_results = json.load(f)\n\n        with\
          \ open(deploy_log_path, 'w') as log_f:\n            log_f.write(\"=== DEPLOYMENT\
          \ LOGS ===\\n\")\n\n            if comparison_results.get('is_better', True):\n\
          \                log_f.write(\"Selecting trained model - it performs better\\\
          n\")\n                log_f.write(f\"Improvement: {comparison_results.get('improvement',\
          \ 0):.2%}\\n\")\n                # Copy trained model to local path\n  \
          \              with open(trained_mar_input.path, 'rb') as src, open(model_path,\
          \ 'wb') as dst:\n                    dst.write(src.read())\n           \
          \ else:\n                log_f.write(\"Selecting base model - trained model\
          \ did not show sufficient improvement\\n\")\n                # Copy base\
          \ model to local path\n                with open(base_mar_input.path, 'rb')\
          \ as src, open(model_path, 'wb') as dst:\n                    dst.write(src.read())\n\
          \n            log_f.write(f\"Reason: {comparison_results.get('message',\
          \ 'No comparison data available')}\\n\")\n\n            # Upload selected\
          \ model to MinIO using the file path\n            try:\n               \
          \ client.fput_object(\n                    minio_bucket,\n             \
          \       \"kserve/model-store/yolo_model.mar\",\n                    str(model_path)\n\
          \                )\n                log_f.write(\"Successfully uploaded\
          \ model to MinIO\\n\")\n            except Exception as e:\n           \
          \     log_f.write(f\"Error uploading model to MinIO: {str(e)}\\n\")\n  \
          \              raise\n\n        # Upload deployment logs to MinIO\n    \
          \    try:\n            client.fput_object(\n                minio_bucket,\n\
          \                \"deployment/deployment_logs.txt\",\n                str(deploy_log_path)\n\
          \            )\n        except Exception as e:\n            print(f\"Error\
          \ uploading deployment logs: {str(e)}\")\n\n    # Configure Kubernetes\n\
          \    kubernetes.config.load_incluster_config()\n    api_instance = kubernetes.client.CustomObjectsApi()\n\
          \n    # Use fixed service definition with correct storageUri path\n    inference_service\
          \ = {\n        \"apiVersion\": \"serving.kserve.io/v1beta1\",\n        \"\
          kind\": \"InferenceService\",\n        \"metadata\": {\n            \"name\"\
          : \"yolov8\",\n            \"namespace\": service_namespace\n        },\n\
          \        \"spec\": {\n            \"predictor\": {\n                \"serviceAccountName\"\
          : \"sa-minio-kserve\",\n                \"model\": {\n                 \
          \   \"modelFormat\": {\n                        \"name\": \"pytorch\"\n\
          \                    },\n                    \"storageUri\": \"s3://mlpipeline/kserve\"\
          ,\n                    \"protocolVersion\": \"v2\"\n                }\n\
          \            }\n        }\n    }\n\n    print(f\"Deploying InferenceService\
          \ 'yolov8' in namespace {service_namespace}\")\n    print(f\"Using storageUri:\
          \ s3://mlpipeline/kserve\")\n\n    try:\n        response = api_instance.create_namespaced_custom_object(\n\
          \            group=\"serving.kserve.io\",\n            version=\"v1beta1\"\
          ,\n            namespace=service_namespace,\n            plural=\"inferenceservices\"\
          ,\n            body=inference_service\n        )\n        print(f\"InferenceService\
          \ created: {response['metadata']['name']}\")\n    except kubernetes.client.rest.ApiException\
          \ as e:\n        if e.status == 409:\n            print(\"InferenceService\
          \ already exists, updating\")\n            response = api_instance.replace_namespaced_custom_object(\n\
          \                group=\"serving.kserve.io\",\n                version=\"\
          v1beta1\",\n                namespace=service_namespace,\n             \
          \   plural=\"inferenceservices\",\n                name=\"yolov8\",\n  \
          \              body=inference_service\n            )\n            print(f\"\
          InferenceService updated: {response['metadata']['name']}\")\n        else:\n\
          \            print(f\"Error creating/updating InferenceService: {e}\")\n\
          \            raise\n\n    print(\"InferenceService has been deployed. Check\
          \ its status manually.\")\n    print(f\"Service will be available at: yolov8.{service_namespace}.example.com\"\
          )\n\n"
        image: python:3.9
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    trained_model_output: Output[Artifact],\n    training_logs_output:\
          \ Output[Artifact],\n    minio_endpoint: str,\n    minio_access_key: str,\n\
          \    minio_secret_key: str,\n    minio_bucket: str,\n    dataset_path: str\
          \ = \"dataset\",\n    base_model: str = \"yolov8n.pt\"\n):\n    import os\n\
          \    import subprocess\n    import tempfile\n    from pathlib import Path\n\
          \    import yaml\n    import shutil\n    from ultralytics import YOLO\n\
          \    from minio import Minio\n\n    client = Minio(\n        minio_endpoint,\n\
          \        access_key=minio_access_key,\n        secret_key=minio_secret_key,\n\
          \        secure=False\n    )\n\n    with tempfile.TemporaryDirectory() as\
          \ temp_dir:\n        temp_dir = Path(temp_dir)\n        dataset_dir = temp_dir\
          \ / \"dataset\"\n        model_dir = temp_dir / \"model\"\n        log_path\
          \ = temp_dir / \"training_logs.txt\"\n        tensorboard_dir = temp_dir\
          \ / \"tensorboard\"\n        tensorboard_dir.mkdir(exist_ok=True)\n\n  \
          \      for subdir in ['images/train', 'images/val', 'labels/train', 'labels/val']:\n\
          \            (dataset_dir / subdir).mkdir(parents=True, exist_ok=True)\n\
          \n        objects = client.list_objects(minio_bucket, prefix=dataset_path,\
          \ recursive=True)\n        for obj in objects:\n            if obj.object_name.endswith('/'):\n\
          \                continue\n            rel_path = obj.object_name[len(dataset_path):].lstrip('/')\n\
          \            if not rel_path:\n                continue\n            local_path\
          \ = dataset_dir / rel_path\n            local_path.parent.mkdir(parents=True,\
          \ exist_ok=True)\n            client.fget_object(minio_bucket, obj.object_name,\
          \ str(local_path))\n\n        yaml_path = dataset_dir / \"data.yaml\"\n\
          \        if yaml_path.exists():\n            with open(yaml_path, 'r') as\
          \ f:\n                data_config = yaml.safe_load(f)\n            data_config['train']\
          \ = str(dataset_dir/'images/train')\n            data_config['val'] = str(dataset_dir/'images/val')\n\
          \            with open(yaml_path, 'w') as f:\n                yaml.dump(data_config,\
          \ f)\n\n        base_model_local = model_dir / base_model\n        client.fget_object(minio_bucket,\
          \ f\"model/{base_model}\", str(base_model_local))\n\n        model = YOLO(str(base_model_local))\n\
          \        results = model.train(\n            data=str(yaml_path),\n    \
          \        epochs=5,\n            imgsz=640,\n            batch=4,\n     \
          \       patience=3,\n            device='cpu',\n            project=str(tensorboard_dir),\
          \  \n            name='',  \n            exist_ok=True,\n            plots=True,\
          \  \n            verbose=True\n        )\n\n        trained_model_local\
          \ = model_dir / \"trained_model.pt\"\n        model.save(str(trained_model_local))\n\
          \n        client.fput_object(\n            minio_bucket,\n            \"\
          model/trained_yolo_model.pt\",\n            str(trained_model_local)\n \
          \       )\n\n        with open(log_path, 'w') as f:\n            f.write(\"\
          === TRAINING COMPLETE ===\\n\")\n            f.write(str(results) + \"\\\
          n\")\n\n        with open(trained_model_output.path, 'wb') as out_f:\n \
          \           out_f.write(open(trained_model_local, 'rb').read())\n\n    \
          \    with open(training_logs_output.path, 'w') as log_art:\n           \
          \ log_art.write(open(log_path, 'r').read())\n\n        # Upload TensorBoard\
          \ files to MinIO\n        for file_path in tensorboard_dir.rglob('*'):\n\
          \            if file_path.is_file():\n                rel_path = file_path.relative_to(tensorboard_dir)\n\
          \                minio_key = f\"tensorboard/{rel_path}\"\n             \
          \   print(f\"Uploading TensorBoard file: {minio_key}\")\n              \
          \  client.fput_object(\n                    minio_bucket,\n            \
          \        minio_key,\n                    str(file_path)\n              \
          \  )\n\n"
        image: lukoprych/yolov8-pipeline-base:latest
        resources:
          cpuRequest: 2.0
          memoryRequest: 4.0
pipelineInfo:
  description: YOLOv8 pipeline
  name: yolov8-pipeline
root:
  dag:
    tasks:
      compare-models:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-compare-models
        dependentTasks:
        - inference-model
        - train
        inputs:
          artifacts:
            trained_model_input:
              taskOutputArtifact:
                outputArtifactKey: trained_model_output
                producerTask: train
          parameters:
            accuracy_threshold:
              runtimeValue:
                constant: 0.03
            base_model_path:
              runtimeValue:
                constant: model/yolov8n.pt
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
            previous_trained_model_path:
              runtimeValue:
                constant: model/trained_yolo_model.pt
        taskInfo:
          name: compare-models
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - package-to-mar
        - train
        inputs:
          artifacts:
            trained_model_input:
              taskOutputArtifact:
                outputArtifactKey: trained_model_output
                producerTask: train
          parameters:
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
        taskInfo:
          name: evaluate-model
      inference-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-inference-model
        dependentTasks:
        - evaluate-model
        - train
        inputs:
          artifacts:
            trained_model_input:
              taskOutputArtifact:
                outputArtifactKey: trained_model_output
                producerTask: train
          parameters:
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
        taskInfo:
          name: inference-model
      package-to-mar:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-package-to-mar
        dependentTasks:
        - train
        inputs:
          artifacts:
            trained_model_input:
              taskOutputArtifact:
                outputArtifactKey: trained_model_output
                producerTask: train
          parameters:
            base_model_path:
              runtimeValue:
                constant: model/yolov8n.pt
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
        taskInfo:
          name: package-to-mar
      serve:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-serve
        dependentTasks:
        - compare-models
        - package-to-mar
        inputs:
          artifacts:
            base_mar_input:
              taskOutputArtifact:
                outputArtifactKey: base_mar_output
                producerTask: package-to-mar
            model_comparison_input:
              taskOutputArtifact:
                outputArtifactKey: model_comparison_output
                producerTask: compare-models
            trained_mar_input:
              taskOutputArtifact:
                outputArtifactKey: trained_mar_output
                producerTask: package-to-mar
          parameters:
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
            service_namespace:
              runtimeValue:
                constant: kubeflow-user-example-com
        taskInfo:
          name: serve
      train:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train
        inputs:
          parameters:
            base_model:
              runtimeValue:
                constant: yolov8n.pt
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
        taskInfo:
          name: train
schemaVersion: 2.1.0
sdkVersion: kfp-2.9.0
